UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

the differences among them, while other metrics cannot.
Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.
Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

Then, we calculate the average similarity precision by:

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.
Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.
Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.
For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.
In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

5. Experiments

5.1. Implementation

4.3. Cross-modal Hard Negative Matching

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.
CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.
Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].
Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

7. Acknowledgement

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.

[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.
First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.
2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.
2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.
2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.
Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.
Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.
Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.
Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.
Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

arXiv:2312.03441v1  [cs.CV]  6 Dec 2023

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

*Corresponding Authors: Yunhe Wang (yunhe.wang@huawei.com),
Changxin Gao (cgao@hust.edu.cn)

This is a young man with a build that looks muscular. His skin is fair,
and he has short black hair. He is wearing a blue baseball cap on his
head and black-framed glasses on his face. His upper body is dressed
in a white pure cotton short-sleeved T-shirt, which just reaches the
position below his waist. His lower body is wearing a long grayish-
milky casual pants, which just reaches the calf position. Meanwhile,
there is something yellow in his right back pants pocket. On his feet
are black sports shoes with white soles. He is carrying a brown
crossbody bag on his back and a white plastic shopping bag with
something red in his hand.
This is a mature male with a tall and sturdy physique. His exposed
skin is white and smooth. He has short, white hair that is roughly at
ear level. He is wearing a white short-sleeved shirt on his upper body
He is holding a red bag in his left hand, and a black briefcase is slung
diagonally over his right shoulder. A black leather belt is worn
around his waist. His lower body is wearing a pair of long, light
khaki-colored casual pants, with the length reaching the heel of his
feet. The man is wearing a pair of black casual shoes on his feet.

Left: A woman wearing a black Jacket
and Leggings holding a bag in her left
hand.
Right: She has shoulder length black
hair. She is also wearing glasses and is
wearing all black clothes .
(a) Coarse-grained
Left: The young man is wearing a black
shirt and black jeans. He has on white
shoes.
Right: The man walking and looking
down at his cellphone is dressed all in
black except for his white shoes.
(b) Ambiguity

Left: A woman wearing a black Jacket
and Leggings holding a bag in her left
hand.
Right: She has shoulder length black
hair. She is also wearing glasses and is
wearing all black clothes .
(a) Coarse-grained
This is a young man with a build that looks muscular. His skin is fair,
and he has short black hair. He is wearing a blue baseball cap on his
head and black-framed glasses on his face. His upper body is dressed
in a white pure cotton short-sleeved T-shirt, which just reaches the
position below his waist. His lower body is wearing a long grayish-
milky casual pants, which just reaches the calf position. Meanwhile,
there is something yellow in his right back pants pocket. On his feet
are black sports shoes with white soles. He is carrying a brown
crossbody bag on his back and a white plastic shopping bag with
something red in his hand.
Left: The young man is wearing a black
shirt and black jeans. He has on white
shoes.
Right: The man walking and looking
down at his cellphone is dressed all in
black except for his white shoes.
This is a mature male with a tall and sturdy physique. His exposed
skin is white and smooth. He has short, white hair that is roughly at
ear level. He is wearing a white short-sleeved shirt on his upper body.
He is holding a red bag in his left hand, and a black briefcase is slung
diagonally over his right shoulder. A black leather belt is worn
around his waist. His lower body is wearing a pair of long, light
khaki-colored casual pants, with the length reaching the heel of his
feet. The man is wearing a pair of black casual shoes on his feet.
(b) Ambiguity

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

Table 1. Some statistics of texts in existing datasets. The text
granularity of ours far exceeds that of others.

Table 1. Some statistics of texts in existing datasets. The text
granularity of ours far exceeds that of others.

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

Then, we calculate the average similarity precision by:

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

3.5. Evaluation Paradigm

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

rank list 1
0.81
0.79
0.77
0.76
0.75
SD = 0.536 
0.81
0.79
0.77
0.15
0.12
SD = 0.744 
rank list 2
0.81
0.79
0.55
0.15
0.12
rank list 3
SD = 0.697

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

Figure 2. A toy example of the difference between SD and AP
metrics. Green and red boxes mean true and false matches, re-
spectively. For these three rank lists, the AP remains 0.833. But
SD = 0.536, 0.744 and 0.697, respectively.

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

PNR = 1 −e−kx,

(1)

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

Lm =
1
|B|
X
(V,W)∈B
(ˆy log p(V, W)+(1−ˆy)(1−log p(V, W))),

(5)

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

Figure 3. Overview of the proposed CFAM framwork.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

V = Dg(Q, V),

(3)

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

(6)

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

v⊺
i wi
Sim(V, W) = 1
K
i
||vi||||wi||.
K
X

(4)

5. Experiments

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Training Sets
CFAM
IRRA [11]
PLIP [52]
R@1
R@5
R@10
mAP
mSD
R@1
R@5
R@10
mAP
mSD
R@1
R@5
R@10
mAP
mSD
CUHK-PEDES
53.80
71.05
78.25
50.40
38.26
50.06
67.98
75.46
47.57
36.50
40.45
57.51
65.20
38.94
30.82
ICFG-PEDES
36.79
54.64
62.93
34.21
25.47
30.57
47.61
55.87
28.38
21.24
34.32
50.52
57.94
32.59
24.88
RSTPReid
29.85
49.08
58.54
29.66
21.82
21.62
39.53
49.38
21.90
16.09
25.25
40.70
48.30
24.62
18.18
UFine6926
64.59
80.16
85.63
60.43
47.76
56.34
72.17
78.47
54.24
42.92
62.84
77.82
83.23
59.31
46.04

Domains
CFAM
IRRA [11]
PLIP [52]
R@1
R@5
R@10
mAP
mSD
R@1
R@5
R@10
mAP
mSD
R@1
R@5
R@10
mAP
mSD
CUHK→UFine
42.49
59.47
68.14
45.06
33.74
37.63
54.99
64.46
40.79
30.80
20.52
33.74
42.69
24.17
18.90
ICFG→UFine
20.65
34.66
43.05
23.09
16.65
14.99
26.85
33.92
17.02
12.29
12.13
21.88
28.73
14.98
11.20
RSTP→UFine
20.20
35.31
44.02
23.13
16.73
13.13
25.59
33.81
15.55
11.20
9.75
18.86
25.27
12.32
8.95
UFine→CUHK
48.72
70.21
78.17
44.42
33.23
41.41
62.72
71.85
39.22
29.86
56.53
77.24
84.10
51.60
39.85
UFine→ICFG
40.78
60.90
69.31
22.30
16.28
35.08
55.16
64.02
18.87
13.85
51.52
70.96
78.02
27.67
20.93
UFine→RSTP
45.10
72.35
81.45
35.40
25.40
41.30
64.25
76.00
32.04
22.93
43.85
72.10
80.60
33.88
24.97

(a) Differences in mutual generalization capabilities between coarse-grained and fine-grained datasets.

A man wearing a white
short-sleeved
shirt
and
black shorts has a black
watch on his wrist.
The woman is wearing a
black clothing and a pair
of black pants, and she
is
wearing
a
pair
of
black glasses.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Methods
Metrics
R@1
R@5
R@10
mAP
mSD
NAFS [10]
43.69
61.34
69.72
39.31
30.32
LGUR [27]
51.26
69.67
75.32
49.22
38.13
SSAN [6]
53.67
71.15
77.15
51.40
39.66
IRRA [11]
56.34
72.17
78.47
54.24
42.92
UFine3C
CFAM(B/16)
58.51
74.92
80.90
55.53
43.22
CFAM(L/14)
62.84
77.82
83.23
59.31
46.04
NAFS [10]
64.11
80.32
85.05
63.47
49.61
LGUR [27]
70.69
84.57
89.91
68.93
56.23
SSAN [6]
75.09
88.63
92.84
73.14
59.41
IRRA [11]
83.53
92.94
95.95
82.79
66.35
UFine6926
CFAM(B/16)
85.55
94.51
97.02
84.23
66.49
CFAM(L/14)
88.51
95.58
97.49
87.09
68.45

Table 7. Comparison with the state-of-the-art methods on RST-
PReid [51]. We show the best score in bold.

No.
Components
CUHK-PEDES
Lgs
Lls
Lm
Lcid
R@1
R@5
R@10
mAP
mSD
0
68.45
86.50
91.68
61.28
46.31
1
✓
70.42
87.20
92.22
63.00
48.38
2
✓
✓
71.69
87.87
92.37
63.85
49.12
3
✓
✓
✓
72.42
88.31
92.80
64.81
49.84
4
✓
✓
✓
✓
72.87
88.61
92.87
64.92
50.20

Table 5. Comparison with the state-of-the-art methods on CUHK-
PEDES [16]. We show the best score in bold.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

Table 8. Ablation study on each component of CFAM.

7. Acknowledgement

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

No.
Components
CUHK-PEDES
share
depth
queries
R@1
R@5
R@10
mAP
mSD
0
1
16
71.17
87.83
92.72
63.83
49.00
1
2
16
71.05
87.56
92.58
63.79
49.10
2
3
16
70.96
87.69
92.31
63.56
48.79
3
4
16
71.72
87.76
92.75
64.13
49.13
4
✓
2
4
72.87
88.61
92.87
64.92
50.20
5
✓
2
8
71.72
88.34
93.00
64.32
49.64
6
✓
2
12
71.61
88.30
92.72
64.27
49.72
7
✓
2
16
71.83
88.43
93.15
64.39
49.52
8
✓
2
20
72.08
88.48
93.02
64.50
49.51
9
✓
3
4
72.09
88.48
93.05
64.44
49.68
10
✓
3
8
71.59
88.61
93.34
64.06
49.06
11
✓
3
12
72.34
88.50
93.00
64.40
49.38
12
✓
3
16
71.85
88.32
92.95
64.23
49.40
13
✓
3
20
72.24
88.55
82.92
64.40
49.64
14
✓
4
4
71.41
88.30
93.02
64.17
49.73
15
✓
4
8
72.13
88.56
93.02
64.32
49.35
16
✓
4
12
71.61
88.32
92.98
64.37
49.63
17
✓
4
16
72.04
88.58
92.97
64.44
49.66
18
✓
4
20
72.06
88.42
93.00
64.41
49.80

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

N


Lid = 1
i
−log
X
,

exp

W ⊤
yixi + byi

j exp

W ⊤
j xi + bj

P

N


Lid = 1
i
−log
X
,

exp

W ⊤
yixi + byi

j exp

W ⊤
j xi + bj

P

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

N


Lcid = 1
i
−log
X

exp

W ⊤
yimlp(zi) + byi

,
(8
j exp

W ⊤
j mlp(zi) + bj

P

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

9. More Results of Ablation Study

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i


1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.


1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.


1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.


1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

10. More Examples of UFineBench

Number
Word Count
Word Count
(a)
(b)
Number
Number
Number
Word Count
Word Count
(c)
(d)

Figure 6. Statistical comparison of the word counts per textual description in our UFine6926 with those in other datasets [6, 16, 51].

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

( ) W
d C
t Di t ib ti

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

(b) Textual Descriptions with Different Granularity

Figure 8. Our UFine3C spans across various text granularity from coarse-grained to fine-grained. The word counts distribution is shown
in (a). The (b) illustrates some specific examples representing three different text granularity conditions.

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

Then, we calculate the average similarity precision by:

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

5. Experiments

5.1. Implementation

4.3. Cross-modal Hard Negative Matching

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

7. Acknowledgement

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

Then, we calculate the average similarity precision by:

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

5. Experiments

5.1. Implementation

4.3. Cross-modal Hard Negative Matching

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

7. Acknowledgement

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

Then, we calculate the average similarity precision by:

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

4.3. Cross-modal Hard Negative Matching

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

5. Experiments

5.1. Implementation

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

7. Acknowledgement

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

Supplementary Material

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

3.2. Dataset with Ultra-fine Granularity

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

Then, we calculate the average similarity precision by:

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

4.3. Cross-modal Hard Negative Matching

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

5. Experiments

5.1. Implementation

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

5.2. Importance of Fine Granularity

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

7. Acknowledgement

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

Supplementary Material

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

Then, we calculate the average similarity precision by:

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

5. Experiments

5.1. Implementation

4.3. Cross-modal Hard Negative Matching

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

7. Acknowledgement

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Abstract

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

the differences among them, while other metrics cannot.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

2. Related Work

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

3.3. Evaluation Set with Cross Settings

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

3.2. Dataset with Ultra-fine Granularity

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

Then, we calculate the average similarity precision by:

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

3.4. A New Evaluation Metric

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

3.5. Evaluation Paradigm

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4.1. Overview

4. Method

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

4.4. Training and Inference Strategy

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

5. Experiments

4.3. Cross-modal Hard Negative Matching

5.1. Implementation

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

5.2. Importance of Fine Granularity

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

5.3. Comparison with State-of-the-Art Methods

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

6. Conclusion

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

5.4. Ablation Study

7. Acknowledgement

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

References

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

Contents

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

8. Cross-Modal Identity Classifying

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

9. More Results of Ablation Study

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

(b) Textual Descriptions with Different Granularity

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Abstract

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Abstract

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Abstract

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.

Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Abstract

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

Then, we calculate the average similarity precision by:

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

4.3. Cross-modal Hard Negative Matching

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

5. Experiments

5.1. Implementation

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

7. Acknowledgement

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

(b) Textual Descriptions with Different Granularity

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

Then, we calculate the average similarity precision by:

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

4.3. Cross-modal Hard Negative Matching

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

5. Experiments

5.1. Implementation

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

7. Acknowledgement

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

(b) Textual Descriptions with Different Granularity

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Jialong Zuo1,2
Hanyu Zhou1
Ying Nie2
Feng Zhang1
Tianyu Guo2
Nong Sang1
Yunhe Wang2∗
Changxin Gao1*
1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab

Abstract

Existing text-based person retrieval datasets often have
relatively coarse-grained text annotations. This hinders the
model to comprehend the fine-grained semantics of query
texts in real scenarios. To address this problem, we con-
tribute a new benchmark named UFineBench for text-based
person retrieval with ultra-fine granularity.
Firstly, we construct a new dataset named UFine6926.
We collect a large number of person images and manually
annotate each image with two detailed textual descriptions,
averaging 80.8 words each.
The average word count is
three to four times that of the previous datasets. In addition
of standard in-domain evaluation, we also propose a spe-
cial evaluation paradigm more representative of real sce-
narios. It contains a new evaluation set with cross domains,
cross textual granularity and cross textual styles, named
UFine3C, and a new evaluation metric for accurately mea-
suring retrieval ability, named mean Similarity Distribution
(mSD). Moreover, we propose CFAM, a more efficient al-
gorithm especially designed for text-based person retrieval
with ultra fine-grained texts. It achieves fine granularity
mining by adopting a shared cross-modal granularity de-
coder and hard negative match mechanism.

With standard in-domain evaluation, CFAM establishes
competitive performance across various datasets, espe-
cially on our ultra fine-grained UFine6926. Furthermore,
by evaluating on UFine3C, we demonstrate that train-
ing on our UFine6926 significantly improves generaliza-
tion to real scenarios compared with other coarse-grained
datasets.
The dataset and code will be made publicly
available at https://github.com/Zplusdragon/
UFineBench.

1. Introduction

Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarse-
grained text annotations in practice. This makes them de-
generated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some ex-
tent.
Considering this, we propose a benchmark named

UFineBench for text-based person retrieval with ultra-fine
granularity, which is more in line with real scenarios.

Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appear-
ance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of com-
plex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identity-
binding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed ex-
planations can be found in Section 3.1.

As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual gran-
ularity and fixed textual styles. However, in real scenar-
ios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, result-
ing from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.

As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked ac-
cording to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretiz-
ing continuous similarity values leads to inaccurate mea-
surement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.

Considering the above three aspects, this paper makes
the following contributions. The first contribution is the

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.

build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descrip-
tions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.

The second contribution is the construction of a spe-
cial evaluation set with cross domains, cross textual gran-
ularity and cross textual styles, named UFine3C, which
is more representative of real scenarios.
It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further en-
rich the variations of textual granularity and styles. It con-
tains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.

As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the con-
tinuous similarity values rather than the discrete rank con-
ditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure

the differences among them, while other metrics cannot.

Based on the proposed cross-modal shared granular-
ity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.

2. Related Work

CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the au-
thors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.

However, the texts in CUHK-PEDES contain identity-
irrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed.
There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to im-
plement semantically self-alignment and part-level feature
automatic extraction.

Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 per-
sons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.

In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.

3. Benchmark

3.1. Granularity Matters

Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained bench-
marks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.

On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based re-
trieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such fine-
grained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granular-
ity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and under-
mines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one iden-
tity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective iden-
tity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.

Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.

3.2. Dataset with Ultra-fine Granularity

We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926.
It contains
26,206 images and 52,412 descriptions of 6,926 persons to-
tally. The construction process is described as two steps:

First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by sta-
tionary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti-

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Fi-
nally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.

Second, to obtain the ultra fine-grained textual descrip-
tions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteris-
tics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person im-
age is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the ex-
amples in Figure 1 and the specific statistics provided in Ta-
ble 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.

In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and un-
fixed scene. It can be served as a benchmark to facilitate
further development in this research field.

3.3. Evaluation Set with Cross Settings

To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First,
we collect the images and textual descrip-
tions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.

Second, as large language models [2, 4, 19–21, 24, 32,
33, 48] show remarkable ability in natural language pro-
cessing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
the description in a different way. You can write it as long or
as short as you like: [original description]”. Meanwhile, we

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.

UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.

3.4. A New Evaluation Metric

Current benchmarks [6, 16, 51] typically use the mean aver-
age precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous sim-
ilarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual simi-
larity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair com-
parison of the quality between these three rank lists.

For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a re-
flection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.

Given a rank list {si}n
i=1 with n ranked samples, where
si means the similarity value of the i-th ranked sample,
which is linearly normalized to the range of 0 to 1, and s+
and s−means respective matched and unmatched samples.
The calculation process of this metric is as follows:

First, we calculate the normalized average similarity ra-
tio between matched samples and unmatched samples by:

where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.

Then, we calculate the average similarity precision by:

where {jk}n+
k=1 means the rankings of n+ matched samples.
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP. Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.

3.5. Evaluation Paradigm

During evaluation, all images in the gallery are ranked ac-
cording to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.

Standard Evaluation. As a standard in-domain evaluation
paradigm, UFine6926 is divided into two subsets for train-
ing and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.

Special Evaluation. As a special evaluation paradigm with
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.

4. Method

4.1. Overview

In
this
section,
we
introduce
a
Cross-modal
Fine-
grained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = {v1, v2, . . . , vni}
and textual embeddings W = {w1, w2, . . . , wnt}, respec-
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval abil-
ity. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.

4.2. Cross-modal Fine-grained Aligning

Given the extracted visual and textual embeddings, most ex-
isting methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to over-
looking the fine-grained details in both modalities. There-
fore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embed-
dings V and textual embeddings W usually have different
length. To address this issue, we propose a shared cross-
modal granularity decoder Dg with a fixed set of granularity

queries Q = {q1, q2, . . . , qK}. These queries can interact
with the embeddings and extract fine-grained information
for cross-modal alignment.

For visual fine-grained information extraction, the gran-
ularity decoder Dg take the queries Q and the visual em-
beddings V as input, and then produce the fine-grained vi-
sual representations as follow,

where V = {v1, v2, . . . , vK} has the same length as the
granularity queries.
Meanwhile, the fine-grained textual
representations W = {w1, w2, . . . , wK} are produced in
the similar way.

In this decoding procedure, the output representations
corresponding to a certain query contain the relevant fine-
grained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the simi-
larity and the overall similarity of the K query outputs can
be calculated by:

Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.

4.3. Cross-modal Hard Negative Matching

To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity represen-
tations of each modality are matched.
This task can be
seen as a binary classification problem: the paired image-
text is considered the positive sample, while the unpaired

is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative repre-
sentations.

For each image within a batch |B|, we sample the un-
paired text whose owns the highest similarity with this im-
age as the hard negative. Also, we sample one hard neg-
ative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:

where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.

4.4. Training and Inference Strategy

As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the repre-
sentations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the sup-
plement. The overall training objective is the weighted sum
of the above losses:

where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.

In the inference phase, we will discard all additional
designs and only compare the similarities between the vi-
sual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a sim-
ilar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the cal-
culated similarities for ranking the image candidates.

5. Experiments

5.1. Implementation

We conduct text-based person retrieval on our pro-
posed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RST-
PReid [51].
Specially, We utilize the UFine3C evalua-
tion set as an extra supplement to assess the generaliza-
tion ability of the models in real scenarios. We adopt the

popular rank-k metric (k=1,5,10), the mean Average Preci-
sion (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.

CFAM mainly consists of a pre-trained visual encoder,
i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,
CLIP textual encoder, a random-initialized granularity de-
coder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer trans-
former blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384×128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmenta-
tion. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e−5. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e−5. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.

5.2. Importance of Fine Granularity

In this section, we conduct experiments to study the impor-
tance of fine granularity in real-world scenarios. Specifi-
cally, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world sce-
narios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA gen-
eralization capability in the current field. However, com-
pared to PLIP, CFAM still demonstrates competitive perfor-
mance under a range of cross settings.

Fineness Better Generalizes to Real-world Scenarios. In
real-world scenarios, there are many variations in image do-
mains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our pro-
posed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than exist-
ing coarse-grained datasets [6, 16, 51], we conduct exper-
iments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models.
Secondly, we

train the models on each training set of the three coarse-
grained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ perfor-
mance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the gen-
eralization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline mod-
els, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training sam-
ples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The re-
sults demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.

Generalization between Fineness and Coarseness. We
conduct the experiments under two aspects. As the first as-
pect, we investigate the differences in mutual generaliza-
tion capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Ta-
ble 3 (a). As we can see, for all of the three baseline mod-
els, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHK-
PEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, train-
ing on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent.
This demon-
strate that training on our fine-grained dataset enables gen-
eralization to coarse-grained datasets, while the reverse
is not true.
As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, train-
ing with our fine-grained dataset is mostly superior to us-
ing other coarse-grained datasets. We choose the coarse-
grained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representa-
tion learning for text-based person retrieval.

Qualitative Results.
To make a more realistic compar-
ison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK-

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.

PEDES [16] and the fine-grained UFine6926 as the base-
line models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to dis-
tinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.

5.3. Comparison with State-of-the-Art Methods

In this section, we compare the performance of our pro-
posed CFAM framework with state-of-the-art (SoTA) meth-
ods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].

Performance Comparisons on UFine6926.
We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set.
We evaluate the per-
formance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each eval-
uation set, CFAM outperforms all other SoTA methods.

Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.

Performance Comparisons on Other Datasets.
The
experimental results on the CUHK-PEDES [16], ICFG-
PEDES [6] and RSTPReid [51] datasets are reported in Ta-
ble 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competi-
tive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.

5.4. Ablation Study

To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]

to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.

6. Conclusion

This paper introduces a new benchmark for text-based per-
son retrieval with ultra-fine granularity.
We firstly con-
tribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new met-
ric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correla-
tion. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient struc-
ture, etc. We believe this work will shed light on more fu-
ture researches in this community.

7. Acknowledgement

This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
gratefully acknowledge the support of MindSpore, CANN
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.

References

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3

[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al.
Qwen technical report.
arXiv preprint
arXiv:2309.16609, 2023. 2, 3
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3
[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8
[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1
[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8
[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3
[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1
[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8
[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakan-
tan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro-
computing, 494:171–181, 2022. 8

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al.
Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3
[5] Mickael Cormier,
Andreas Specker,
Julio Junior,
CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moes-
lund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Bey-
erer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao.
Semantically self-aligned network for text-to-
image part-aware person re-identification.
arXiv preprint
arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsa-
pr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
Journal of Ambient Intelligence and Humanized Computing,
pages 1–13, 2022. 3
[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun.
Contextual non-local alignment over full-scale rep-
resentation for text-based person search.
arXiv preprint
arXiv:2101.03036, 2021. 8

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation rea-
soning and aligning for text-to-image person retrieval.
In
CVPR, 2023. 1, 2, 4, 5, 6, 7, 8

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 6
[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang.
Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3
[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.
[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3
[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8
[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8
[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8
[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3
[20] OpenAI. Gpt-4 technical report, 2023.
[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3
[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3
[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8
[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3
[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8
[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3
[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation.
In ICML,
pages 12888–12900. PMLR, 2022.

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2:
bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural lan-
guage description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Im-
proving description-based person re-identification by multi-
granularity image-text alignments.
IEEE TIP, 29:5542–
5556, 2020. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language.
In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI.
Chatgpt.
https://openai.com/blog/
chatgpt/, 2023. 3

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-
roll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al.
Training lan-
guage models to follow instructions with human feedback.
NeurIPS, 35:27730–27744, 2022. 3

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
Learning Research, 21(1):5485–5551, 2020. 3

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image match-
ing. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video se-
quences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
ACM MM, pages 5566–5574, 2022. 8

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1
[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8
[30] Andreas Specker and J¨urgen Beyerer. Improving attribute-
based person retrieval by using a calibrated, weighted, and
distribution-based distance metric.
In ICIP, pages 2378–
2382. IEEE, 2021. 3
[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person re-
trieval. In WACV, pages 981–990, 2023. 3
[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto.
Stanford alpaca:
An instruction-following
llama model. https://github.com/tatsu-lab/
stanford_alpaca, 2023. 3
[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Mar-
tinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3
[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
preprint arXiv:2307.09288, 2023. 2, 3
[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
information processing systems, 30, 2017. 6
[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video re-
trieval with local alignment and fine-grained supervision.
IEEE TMM, 2022. 3
[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute re-
trieval. International Journal of Multimedia Information Re-
trieval, 12(2):35, 2023. 1
[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vi-
taa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8
[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round infor-
mation beyond color for text-based person retrieval. In ACM
MM, pages 5314–5322, 2022. 8
[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improv-
ing text-based person retrieval by learning a consistent cross-
modal common manifold. In ACM MM, pages 1984–1992,
2022. 8
[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person re-
identification. In CVPR, pages 79–88, 2018. 2

[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: language-
guided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8
[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
IEEE TIP, 2023. 8
[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3
[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1
[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and re-
identification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3
[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jing-
dong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2
[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonza-
lez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3
[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled sam-
ples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2
[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac-
tions on Multimedia Computing, Communications, and Ap-
plications (TOMM), 16(2):1–23, 2020. 8
[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua.
Dssl: Deep
surroundings-person separation learning for text-based per-
son retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8
[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8

UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity

Supplementary Material

Contents

In this supplementary material, we will 1) show the de-
tails of our proposed cross-modal identity classifying loss
Lcid, 2) show more results of the ablation study, and 3)
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].

8. Cross-Modal Identity Classifying

In the training phase of CFAM, we also propose the cross-
modal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same im-
age/text pair are closely clustered together.

First, referring to [45], we revisit the traditional iden-
tity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}N
i=1 and the iden-
tity labels Y = {yi}N
i=1, the traditional identity loss can be
computed by:

where W yi and W j denote the yi-th and j-th column of
classification weight matrix W , yi indicates the identity la-
bel of xi, and byi and bj represent the yi-th and j-th element
of bias vector.

However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the cross-
modal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard neg-
ative unmatched sample pairs.

Given the extracted visual embeddings V = {vi}N
i=1
and textual embeddings T = {ti}N
i=1, for each vi within
V, we sample the unpaired textual embedding which owns
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
ti within T in the same way. Specially, we add an extra
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N| original positive pairs with original identity
labels and 2|N| negative pairs with an unmatched label, de-
noted as |B| pairs with the identity labels {yi}B
i=1.i

Then, for the |B| pairs {vi, ti, yi}B
i=1, we first concate-
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for {zi}B
i=1

Table 9.
Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.

with the identity labels {yi}B
i=1, the cross-modal identity
classifying loss can be computed by:

where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the cross-
modal embeddings more deeply.

9. More Results of Ablation Study

We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.

2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.

1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.

2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.

1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.

2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.

1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.
2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.

1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.

2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.

Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.

10. More Examples of UFineBench

Ultra Fine-grained UFine6926.
We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person im-
age is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine-

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description ac-
curately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.

Three Cross Settings in UFine3C. Our proposed UFine3C
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the chal-
lenges faced in real scenarios.
(1) Our UFine3C spans
across various domains. As shown in Figure 7, the images
in UFine3C have significant variations in resolution, illumi-
nation and shooting scenes, which is very close to the situa-
tion of images obtained in real scenarios. (2) Our UFine3C
spans across various text granularity. As shown in Figure 8,
on the left, the word counts distribution of UFine3C is span-
ning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon-

sistency of granularity within the query texts in real scenar-
ios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.

Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.

Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.

Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white
pants and a pair of white shoes.

Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.
Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.

Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.

Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.

Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.

Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.

(b) Textual Descriptions with Different Granularity

