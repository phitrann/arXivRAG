### UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity


#### Jialong Zuo[1][,][2] Hanyu Zhou[1] Ying Nie[2] Feng Zhang[1] Tianyu Guo[2] Nong Sang[1]
 Yunhe Wang[2][∗] Changxin Gao[1][*]

1National Key Laboratory of Multispectral Information Intelligent Processing Technology,
#### School of Artificial Intelligence and Automation, Huazhong University of Science and Technology
2Huawei Noah’s Ark Lab


#### Abstract


_Existing text-based person retrieval datasets often have_
_relatively coarse-grained text annotations. This hinders the_
_model to comprehend the fine-grained semantics of query_
_texts in real scenarios. To address this problem, we con-_
_tribute a new benchmark named UFineBench for text-based_
_person retrieval with ultra-fine granularity._
_Firstly, we construct a new dataset named UFine6926._
_We collect a large number of person images and manually_
_annotate each image with two detailed textual descriptions,_
_averaging 80.8 words each._ _The average word count is_
_three to four times that of the previous datasets. In addition_
_of standard in-domain evaluation, we also propose a spe-_
_cial evaluation paradigm more representative of real sce-_
_narios. It contains a new evaluation set with cross domains,_
_cross textual granularity and cross textual styles, named_
_UFine3C, and a new evaluation metric for accurately mea-_
_suring retrieval ability, named mean Similarity Distribution_
_(mSD). Moreover, we propose CFAM, a more efficient al-_
**_gorithm especially designed for text-based person retrieval_**
_with ultra fine-grained texts. It achieves fine granularity_
_mining by adopting a shared cross-modal granularity de-_
_coder and hard negative match mechanism._


_With standard in-domain evaluation, CFAM establishes_
_competitive performance across various datasets, espe-_
_cially on our ultra fine-grained UFine6926. Furthermore,_
_by evaluating on UFine3C, we demonstrate that train-_
_ing on our UFine6926 significantly improves generaliza-_
_tion to real scenarios compared with other coarse-grained_
_datasets._ _The dataset and code will be made publicly_
_[available at https://github.com/Zplusdragon/](https://github.com/Zplusdragon/UFineBench)_
_[UFineBench.](https://github.com/Zplusdragon/UFineBench)_


#### 1. Introduction


Existing text-based person retrieval benchmarks [6, 16, 51],
even if claimed to be fine-grained, often have coarsegrained text annotations in practice. This makes them degenerated into attribute-based retrieval [5, 8, 28, 37] due
to the provided coarse-grained descriptions to some extent. Considering this, we propose a benchmark named


_UFineBench for text-based person retrieval with ultra-fine_
granularity, which is more in line with real scenarios.


Our work is motivated by three main aspects. As the first
aspect, existing datasets [6, 16, 51] suffer from a common
issue that the text is not fine-grained enough to effectively
apply to real scenarios. Specifically, as shown in Figure 1
(a), they almost only briefly describe the common appearance of persons, and lack further specific descriptions of
the unique appearance. This can easily lead to the model
only being able to identify typical attribute characteristics
and cannot understand the fine-grained semantics of complex query texts in real scenarios. Meanwhile, as shown in
Figure 1 (b), they suffer from the ambiguity of one identitybinding text corresponding to multiple different identities,
hindering the model from accurately understanding how
texts and images match during training. The detailed explanations can be found in Section 3.1.


As the second aspect, existing standard evaluation
sets [6, 16, 51] all have fixed domain, fixed textual granularity and fixed textual styles. However, in real scenarios, there are usually three common features. 1) Extensive
time and location coverage of surveillance videos, leading
to substantial domain variations within the image gallery; 2)
Inconsistency of granularity within the query texts, resulting from the variability in the actual information available
for the person being searched; 3) The language expression
of each describer has an unique style, even when conveying
the same meaning. However, existing evaluation sets with
fixed settings are inadequate for effectively assessing the
model’s performance in real scenarios with these features.


As the third aspect, existing evaluation metrics [11, 16]
are not accurate enough to measure the retrieval ability.
Given a text query, all images in the gallery are ranked according to their similarities with the query. The commonly
used rank-k metric is calculated according to whether any
image of the corresponding person is retrieved among the
top k images. However, this calculation method of discretizing continuous similarity values leads to inaccurate measurement. For example, for the same rank conditions, the
similarity conditions are highly likely to be different, but
the rank-k metric cannot measure such differences.


Considering the above three aspects, this paper makes
the following contributions. The first contribution is the


-----

Figure 1. Comparisons between our proposed UFine6926 and existing other datasets. (a)-(b) are the examples from CUHK-PEDES [16].
In (a), some fine-grained features not described in the text are highlighted in red boxes. In (b), the text does not provide enough details to
closely match its intended identity but effectively describes the other identity. Meanwhile, two examples from UFine6926 are presented
on the right, with ultra fine-grained texts. As the text details some fine-grained features in the images (highlighted in different colors
correspondingly), it not only provides rich cross-modal information but also effectively distinguishes highly similar image samples.


build of a high quality dataset with ultra-fine granularity for
text-based person retrieval, named UFine6926. It contains
6,926 identities, 26,206 images and 52,412 textual descriptions. A total of 58 annotators participated in crafting the
textual descriptions. Each annotator is required to provide a
description according to the person’s appearance as detailed
as possible. Compared to existing datasets [6, 16, 51], the
UFine6926 dataset has significant superiority in terms of
textual granularity. As shown in Figure 1, the level of detail
in each textual description has significantly improved. The
average word count is 80.8 and three to four times that of
the previous datasets.


The second contribution is the construction of a special evaluation set with cross domains, cross textual granularity and cross textual styles, named UFine3C, which
is more representative of real scenarios. It is collected
from the test sets of the coarse-grained CUHK-PEDES [16],
the medium-grained ICFG-PEDES [6] and our fine-grained
UFine6926 to contain different domains, textual granularity
and textual styles. Meanwhile, we utilize the large language
models Qwen-14B [1] and Llama2-70B [34] to further enrich the variations of textual granularity and styles. It contains 7,446 images to be searched and 37,939 text queries
of 2,250 persons in total.


As the third contribution, a more accurate evaluation
metric is proposed for measuring retrieval ability, named
mean Similarity Distribution (mSD). It is based on the continuous similarity values rather than the discrete rank conditions [11, 16, 47, 49]. It requires the model to distinguish
as much as possible the similarity differences between text
queries and positive-negative image samples within a more
precise numerical range. For the same rank conditions with
different similarity conditions, it can sensitively measure


the differences among them, while other metrics cannot.


Based on the proposed cross-modal shared granularity decoder and hard negative match mechanism, we also
contribute a novel Cross-modal Fine-grained Aligning and
Matching framework (CFAM). It establishes competitive
performance on various datasets without bells and whistles,
especially on our fine-grained UFine6926.


#### 2. Related Work


CUHK-PEDES [16] is the first benchmark focusing on
text-based person retrieval. It contains 40,206 images and
80,412 texts of 13,003 identities. The average word count
per text is 23.5. To provide a baseline algorithm, the authors propose GNA-RNN which introduces the gated neural
attention mechanism into an recurrent neural network.


However, the texts in CUHK-PEDES contain identityirrelevant details. To address this issue, the ICFG-PEDES
benchmark [6] is constructed. There are 54,522 person
images, 54,522 texts of 4,102 identities gathered from
MSMT17 [41]. The average word count per text is is 37.2.
As a baseline algorithm, the authors propose SSAN to implement semantically self-alignment and part-level feature
automatic extraction.


Meanwhile, RSTPReid [51] is constructed. It contains
20,505 images and 51,010 textual descriptions of 4,101 persons totally. The average word count per text is is 26.5. As a
baseline algorithm, the authors propose DSSL which takes
surroundings-person separation, fusion mechanism and five
alignment paradigms into a unified framework.


In short, these existing benchmarks are all suffering from
coarse textual granularity. Therefore, it is necessary for us
to propose a benchmark with ultra-fine granularity.


-----

#### 3. Benchmark


##### 3.1. Granularity Matters


Granularity-related research has become a hot topic in
the computer vision field [13–15, 22, 36, 44]. However,
when directing to text-based person retrieval, researchers
often confine themselves to a few coarse-grained benchmarks [6, 16, 51], thereby overlooking the significance of
granularity in practical applications. We believe that the
coarseness of textual granularity in existing benchmarks can
give rise to the following two issues.


On the one hand, a substantial amount of coarse-grained
descriptions greatly degrade the task into attribute-based retrieval [9, 26, 30, 31]. A simple example is illustrated in
Figure 1 (a). Since the text does not describe such finegrained features highlighted in red boxes, the model can
not understand what brown boots, white stripes, and so on,
refer to. When facing real scenarios with highly detailed
text queries, the models trained on such coarse-grained data
often prove inadequate. Meanwhile, searching for images
based on coarse-grained attributes is what attribute-based
person retrieval excel at. Therefore, coarse textual granularity makes these two tasks being fundamentally equivalent.
On the other hand, coarse textual granularity introduces
significant ambiguity into the training process and undermines the model’s performance. As a standard practice,
the optimization objectives are based on the premise that
each text is only associated with the images of one identity. However, in the existing benchmarks [6, 16, 51], it is
common that one text can be used to describe the images
from different identities. A simple example is illustrated in
Figure 1 (b). Due to the overly coarseness, the text of each
images cannot be highly correlated to its respective identity. Instead, it describes the other identity quite well. This
ambiguity significantly hinder the model from accurately
understanding how texts and images match during training.


Consequently, we emphasize that the text granularity
matters and is a non-ignorable factor for text-based person
retrieval. Motivated by this, we propose this benchmark
with ultra-fine granularity in textual descriptions.


##### 3.2. Dataset with Ultra-fine Granularity


We construct the first high quality dataset with ultra-fine
granularity for this task, named UFine6926. It contains
26,206 images and 52,412 descriptions of 6,926 persons totally. The construction process is described as two steps:


First, while the person images in existing datasets are
mostly derived from fixed-scene videos captured by stationary cameras, our dataset leverages a vast collection of
unrestricted scene videos from the internet to obtain these
images. We utilize the FairMOT algorithm [46] to extract
person tracklets from the scene videos provided by [7]. One
person tracklet is considered as one identity. Then, we uti

lize the noise-filtering strategies proposed in PLIP [52] to
perform preliminary denoising on the obtained images. Finally, we conduct meticulous manual selection to ensure the
image quality. Through this procedure, we have collected
26,206 high quality images of 6,926 identities in total.


Second, to obtain the ultra fine-grained textual descriptions, we hire 58 unique workers involved in the annotation
task, instructing them to describe all important characteristics in the given images as detailed as possible. There are a
total of 8,475 unique words in our dataset. Each person image is annotated with two textual descriptions. The longest
description has 218 words and the average word count is
80.8, which is significantly larger than the 23.5 words of
CUHK-PEDES [16], 37.2 words of ICFG-PEDES [6] and
26.5 words of RSTPReid [51]. As demonstrated by the examples in Figure 1 and the specific statistics provided in Table 1, our dataset exhibits a significant advantage in terms
of textual granularity when compared to existing datasets.


In conclusion, the properties of our UFine6926 dataset
can be summarized as follows: ultra fine-grained and unfixed scene. It can be served as a benchmark to facilitate
further development in this research field.


##### 3.3. Evaluation Set with Cross Settings


To better evaluate the model performance in real scenarios,
we construct a evaluation set named UFine3C with cross
domains, cross textual granularity and cross textual styles
based on two existing datasets [6, 16] and our UFine6926.
This evaluation set is very challenging and the construction
process is described as two steps:
First, we collect the images and textual descriptions of according persons from the test sets of the
coarse-grained “CUHK-PEDES” [16], the medium-grained
“ICFG-PEDES” [6] and our fine-grained “UFine6926”. We
collect 750 persons from each of them to avoid bias and
ensure fairness. After this collection, we obtain a set with
spanning domains, textual granularity and textual styles.


Second, as large language models [2, 4, 19–21, 24, 32,

33, 48] show remarkable ability in natural language processing, we utilize Qwen-14B [1] and Llama2-70B [34]
to further enrich the variations of textual granularity and
styles. Given an original description, we ask the models
to response with the prompt instruction: “Please reorganize
_the description in a different way. You can write it as long or_
_as short as you like: [original description]”. Meanwhile, we_


-----

manually revise the responses generated by them to avoid
incorrect answers. Through this approach, we obtain more
textual descriptions with different styles and granularity.


UFine3C contains 7,446 images, 37,939 text queries of
2,250 persons totally. This evaluation set with cross settings
is more consistent with real scenarios and can be served as
a standard evaluation set to facilitate relevant researches.


##### 3.4. A New Evaluation Metric


Current benchmarks [6, 16, 51] typically use the mean average precision (mAP) to evaluate the overall performance of
person retrieval algorithms. This evaluation metric is based
on discrete rank conditions and cannot sensitively measure
the differences in model performance at a continuous similarity level. However, continuous similarity values more
realistically reflect the model’s retrieval ability. As seen in
Figure 2, there is a significant difference in the actual similarity values of these three rank lists. However, the APs of
them both equal to 0.833, which fail to provide a fair comparison of the quality between these three rank lists.


For UFine6926 dataset, the fine-grained retrieval ability
is what we especially emphasize and any difference is a reflection of it. Therefore, we propose a new metric named
mean similarity distribution (mSD) to evaluate the overall
performance at a continuous similarity level. As shown in
Figure 2, when mSD is used, the differences between these
three rank lists can be well distinguished. The SDs of them
are 0.536, 0.744 and 0.697, respectively.


Given a rank list {si}i[n]=1 [with][ n][ ranked samples, where]
_si means the similarity value of the i-th ranked sample,_
which is linearly normalized to the range of 0 to 1, and s[+]

and s[−] means respective matched and unmatched samples.
The calculation process of this metric is as follows:


First, we calculate the normalized average similarity ratio between matched samples and unmatched samples by:


where x is the average similarity ratio between matched and
unmatched samples in a list and k is set to 1 as default.


Then, we calculate the average similarity precision by:


where _jk_ _k=1_ [means the rankings of][ n][+][ matched samples.]
_{_ _}[n][+]_
Then, the similarity distribution (SD) of a rank list can
be measured by the product of PNR and ASP . Finally, the
mean value of SDs of all rank lists, i.e., mSD, is calculated
as our evaluation metric.


##### 3.5. Evaluation Paradigm


During evaluation, all images in the gallery are ranked according to their similarities with the text query. We adopt
the traditional rank-k accuracy and mAP, and our newly
proposed mSD to evaluate the retrieval performance.


**Standard Evaluation. As a standard in-domain evaluation**
paradigm, UFine6926 is divided into two subsets for training and test. The training set contains 18,577 images and
37,154 texts of 4926 identities. The test set contains 7,629
images and 15,258 text queries of 2,000 identities.


**Special Evaluation. As a special evaluation paradigm with**
cross settings, UFine3C is utilized as a test set for evaluating
the model performance in real scenarios. The training set is
the same as that of standard paradigm.


#### 4. Method


##### 4.1. Overview


In this section, we introduce a Cross-modal Finegrained Aligning and Matching framework (CFAM), which
achieves fine granularity mining in a non trivial way. The
whole framework is shown in Figure 3, given an input
image I and an input text T, the CLIP [23] pre-trained
visual encoder Ev and textual encoder Et are adopted
to extract the visual embeddings V = **v1, v2, . . ., vni**
_{_ _}_
and textual embeddings W = **w1, w2, . . ., wnt**, respec_{_ _}_
tively. Specially, we design a cross-modal fine-grained align
and match module to improve the fine-grained retrieval ability. Through a shared cross-modal granularity decoder and
hard negative match mechanism, the framework achieves
competitive performance on various datasets and settings.


##### 4.2. Cross-modal Fine-grained Aligning


Given the extracted visual and textual embeddings, most existing methods [11, 52] only calculate global similarities to
achieve cross-modal alignment, which is inclined to overlooking the fine-grained details in both modalities. Therefore, we propose to perform more fine-grained alignment
based on the local embeddings. However, the visual embeddings V and textual embeddings W usually have different
length. To address this issue, we propose a shared crossmodal granularity decoder Dg with a fixed set of granularity


-----

queries Q = **q1, q2, . . ., qK** . These queries can interact
_{_ _}_
with the embeddings and extract fine-grained information
for cross-modal alignment.


For visual fine-grained information extraction, the granularity decoder Dg take the queries Q and the visual embeddings V as input, and then produce the fine-grained visual representations as follow,


where V = **v1, v2, . . ., vK** has the same length as the
_{_ _}_
granularity queries. Meanwhile, the fine-grained textual
representations W = **w1, w2, . . ., wK** are produced in
_{_ _}_
the similar way.


In this decoding procedure, the output representations
corresponding to a certain query contain the relevant finegrained information from both modalities and share similar
semantic content. Therefore, the cross-modal similarity for
each query output can be measured to achieve fine-grained
alignment. We use the cosine distance to measure the similarity and the overall similarity of the K query outputs can
be calculated by:


Then, given a batch of B image-text pairs, the commonly
used SDM loss [11] will be utilized to calculate the local
alignment loss Lls according to the similarity distribution.


##### 4.3. Cross-modal Hard Negative Matching


To further facilitate the cross-modal alignment, we propose
to perform prediction on whether the granularity representations of each modality are matched. This task can be
seen as a binary classification problem: the paired imagetext is considered the positive sample, while the unpaired


is considered the negative one. Unlike common random
sampling, we employ the hard negative mining strategy,
which is beneficial to learning more discriminative representations.


For each image within a batch |B|, we sample the unpaired text whose owns the highest similarity with this image as the hard negative. Also, we sample one hard negative image for each text in the same way. Through this
approach, we obtain |B| positive pairs and 2|B| negative
pairs, denoted as |B| pairs. Then, we pass the fine-grained
representations of these |B| pairs through a binary classifier
named Matcher, to optimize the following objective:


where p is a binary likelihood distribution function, and ˆy is
1 if (V, W) is matched, 0 otherwise.


##### 4.4. Training and Inference Strategy


As complementary to fine-grained alignment, we compute
the global similarity between the global visual embedding
and the global textual embedding, and optimize the global
alignment loss Lgs according to it. Also, we propose to
utilize the cross-modal identity classifying loss Lcid with
hard negative samples to explicitly ensure that the representations of the same image/text pair are closely clustered
together. The details of this strategy are shown in the supplement. The overall training objective is the weighted sum
of the above losses:


where λ1, λ2, λ3 are hyper-parameters to adjust the weight
of each loss, which are all set to 1 as the default.


In the inference phase, we will discard all additional
designs and only compare the similarities between the visual and textual global embeddings. First, all images in
the gallery will be passed to the visual encoder to extract
the according global visual embeddings. Second, for each
text query, we obtain its textual global embedding in a similar way and then we compute its similarity with the visual
global embeddings of all images. Finally, we utilize the calculated similarities for ranking the image candidates.


#### 5. Experiments


##### 5.1. Implementation


We conduct text-based person retrieval on our proposed fine-grained UFine6926 datasets and three existing
datasets CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. Specially, We utilize the UFine3C evaluation set as an extra supplement to assess the generalization ability of the models in real scenarios. We adopt the


-----

popular rank-k metric (k=1,5,10), the mean Average Precision (mAP) and our proposed mean Similarity Distribution
(mSD) as the evaluation metrics. The higher rank-k, mAP
and mSD indicates better performance.


CFAM mainly consists of a pre-trained visual encoder,
_i.e., CLIP-ViT-B/16 [23], a pre-trained text encoder, i.e.,_
CLIP textual encoder, a random-initialized granularity decoder and a matcher. The granularity decoder is shared by
visual and textual modalities, consisting of 2-layer transformer blocks [35]. The matcher is consisted of 2-layer
transformer blocks and an MLP with sigmoid activation.
For each layer of the granularity decoder and matcher, the
hidden size and number of heads are set to 512 and 8. The
number of granularity queries is set to 16 and their hidden
dimension is 512. For downstream training, the images are
resized to 384 _×_ 128 and the maximum length of the textual
tokens is set to 168. The batchsize per GPU is set to 64.
Also, random erasing, horizontally flipping and crop with
padding are employed for image augmentation. Random
masking and replacement is employed for text augmentation. Our CFAM is trained with Adam [12] for 60 epochs
with an initial learning rate 1e[−][5]. We adopt the linearly
warm-up strategy within the beginning 5 epochs. For the
random-initialized modules, the initial learning rate is set to
5e[−][5]. We adopt the cosine learning rate decay strategy. The
experiments are performed on 1 V100 32GB GPU.


##### 5.2. Importance of Fine Granularity


In this section, we conduct experiments to study the importance of fine granularity in real-world scenarios. Specifically, we have trained two baseline models (PLIP [52] and
IRRA [11]) and our CFAM on three coarse-grained existing
datasets and our fine-grained dataset. Then, due to the fact
that generalization ability is indispensable in real-world scenarios, we evaluate their performance under a range of cross
settings. Please note that PLIP [52] is a pre-trained model
on a large amount of pedestrian data, giving it a SoTA generalization capability in the current field. However, compared to PLIP, CFAM still demonstrates competitive performance under a range of cross settings.


**Fineness Better Generalizes to Real-world Scenarios. In**
real-world scenarios, there are many variations in image domains, textual granularity and textual styles. The ability
to effectively address these variations is a necessity for a
high-level model. To study whether training on our proposed fine-grained UFine6926 dataset can lead the model
to better generalize to the real-world scenarios than existing coarse-grained datasets [6, 16, 51], we conduct experiments by setting the UFine3C evaluation set as the target
set to be transfered. The specific experimental procedure
is described as follows. Firstly, We choose two existing
popular open-source and state-of-the-art methods [11, 52]
and our CFAM as the baseline models. Secondly, we


train the models on each training set of the three coarsegrained datasets [6, 16, 51] and our fine-grained UFine6926.
Thirdly, we directly evaluate the trained models’ performance on the UFine3C evaluation set. By comparing the
performance differences, we can effectively assess the generalization ability of models trained on various datasets to
real-world scenarios. The experimental results are reported
in Table 2. As we can see, for all the three baseline models, training on our UFine6926 dataset will significantly
lead to better performance on the UFine3C evaluation set.
Specifically, CFAM achieves 64.59%, 80.16%, 85.63% and
47.76% on rank-1, rank-5, rank-10 and mSD, respectively,
greatly exceeding the results obtained by training on other
coarse-grained datasets. We must note that the training samples in UFine6926 is much less than that in other datasets,
while still achieves state-of-the-art performance. The results demonstrate that our fine-grained UFine6926 helps
to learn more discriminative and general representations,
which is beneficial to generalize to the real scenarios.


**Generalization between Fineness and Coarseness. We**
conduct the experiments under two aspects. As the first aspect, we investigate the differences in mutual generalization capabilities between coarse-grained and fine-grained
datasets. We train the models on the coarse-grained datasets
and then directly transfer them to our fine-grained dataset,
and vice versa. The experimental results are reported in Table 3 (a). As we can see, for all of the three baseline models, training on the coarse-grained datasets cannot well be
transferred to our fine-grained dataset. For example, when
transferring to UFine6926, PLIP [52] trained on CUHKPEDES [16] only achieves 20.52%, 33.74%, 42.69% on
rank-1, rank-5 and rank-10, respectively, which falls far
short of practical application requirements. However, training on our fine-grained dataset can be transferred to the
coarse-grained datasets to a better extent. This demonstrate that training on our fine-grained dataset enables generalization to coarse-grained datasets, while the reverse
is not true. As the second aspect, we demonstrate that
even when transferring to a coarse-grained dataset, training with our fine-grained dataset is mostly superior to using other coarse-grained datasets. We choose the coarsegrained ICFG-PEDES [6] and RSTPReid [51] datasets as
the target datasets. As the results reported in Table 3 (b),
for all of the baselines, even though our dataset contains
very little coarse-grained data, training on our UFine6926
still achieves better or competitive performance on the two
target coarse-grained datasets. All the results demonstrate
that our fine-grained dataset improves general representation learning for text-based person retrieval.


**Qualitative Results.** To make a more realistic comparison of the models’ performance in real-world scenarios,
we conduct a straightforward qualitative experiment. We
choose our CFAM trained on the coarse-grained CUHK

-----

Table 3. Performance comparisons on the generalization performance between our fine-grained UFine6926 and three existing datasets
CUHK-PEDES [16], ICFG-PEDES [6] and RSTPReid [51]. The arrow direction indicates the source dataset and the target dataset.


PEDES [16] and the fine-grained UFine6926 as the baseline models to be compared. Then, we manually provide
any textual descriptions to search the according persons in
the UFine3C dataset. The rank-10 retrieval results from the
CFAM models trained on CUHK-PEDES and UFine6926
respectively are compared in Figure 4. As it shows, training
on UFine6926 achieves more accurate retrieval results and
can fully perceive fine-grained discriminative clues to distinguish different persons, while training on CUHK-PEDES
fails to do so. This is illustrated in the orange highlighted
text and image regions box in Figure 4.


##### 5.3. Comparison with State-of-the-Art Methods


In this section, we compare the performance of our proposed CFAM framework with state-of-the-art (SoTA) methods on our fine-grained UFine6926 dataset and three public
coarse-grained datasets [6, 16, 51].


**Performance Comparisons on UFine6926.** We utilize
two evaluation sets for the performance comparison on
UFine6926. The first is the UFine3C evaluation set. The
second is the UFine6926 test set. We evaluate the performance of existing SoTA methods trained on our new
UFine6926. As the results shown in Table 4, on each evaluation set, CFAM outperforms all other SoTA methods.


Specifically, with CLIP-ViT-L/14 [23] setting, it achieves
62.84% rank-1 accuracy, 59.31% mAP and 46.04% mSD
on UFine3C, respectively. Meanwhile, it achieves 88.51%
rank-1 accuracy, 57.09% mAP and 68.45% mSD on
UFine6926 test set, respectively. These results demonstrate
the superior fine-grained retrieval capability of CFAM.


-----

**Performance Comparisons on Other Datasets.** The
experimental results on the CUHK-PEDES [16], ICFGPEDES [6] and RSTPReid [51] datasets are reported in Table 5, Table 6 and Table 7, respectively. On CUHK-PEDES,
with the CLIP-ViT-B/16 setting, CFAM achieves competitive results to recent state-of-the-art methods without bells
and whistles, achieving 72.87% rank-1 accuracy, 64.92%
mAP and 50.20% mSD, respectively. Meanwhile, with the
CLIP-ViT-L/14 setting, the performance of CFAM can be
further improved, achieving 75.60% rank-1, 67.27% mAP
and 51.83% mSD, respectively. This means that our method
has good scalability. On ICFG-PEDES and RSTPReid, our
CFAM also outperforms all state-of-the-art methods by a
considerable margin. It achieves 65.38% rank-1, 39.42%
mAP and 30.29% mSD on ICFG-PEDES, 62.45% rank-1,
49.50% mAP and 36.92% mSD on RSTPReid, respectively.
The results demonstrate that CFAM helps to learn general
representations on various datasets.


##### 5.4. Ablation Study


To verify the contribution of each component in CFAM,
we conduct an ablation experiment on CUHK-PEDES
dataset [16]. The results are reported in Table 9. No.0 is the
baseline utilizing the original InfoNCE loss in CLIP [23]


to align the cross-modal global embeddings. As we can
see, each component facilitates the model’s capability, and
combining all of them leads to the best performance. In
addition, we have conducted further ablation experiments,
which are detailed in the supplement.


#### 6. Conclusion


This paper introduces a new benchmark for text-based person retrieval with ultra-fine granularity. We firstly contribute a manually annotated dataset named UFine6926 with
ultra fine-grained texts. Meanwhile, we propose a special
evaluation paradigm more representative for real scenarios
with a new evaluation set named UFine3C and a new metric named mSD. Then, CFAM is proposed in the attempt
to achieve fine-grained cross-modal representation correlation. Our benchmark will enable research possibilities in
multiple directions, e.g., fine-grained retrieval, real scenario
generalization, multi-granularity adaptation, efficient structure, etc. We believe this work will shed light on more future researches in this community.


-----

#### 7. Acknowledgement


This work was supported by the National Natural Science
Foundation of China No.62176097, and Hubei Provincial
Natural Science Foundation of China No.2022CFA055. We
[gratefully acknowledge the support of MindSpore, CANN](https://www.mindspore.cn/)
(Compute Architecture for Neural Networks) and Ascend
AI Processor used for this research.


#### References



[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. Qwen technical report. _arXiv preprint_
_arXiv:2309.16609, 2023. 2, 3_



[1] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
Huang, et al. Qwen technical report. _arXiv preprint_
_arXiv:2309.16609, 2023. 2, 3_

[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3

[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro_computing, 494:171–181, 2022. 8_

[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3

[5] Mickael Cormier, Andreas Specker, Julio Junior, CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moeslund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Beyerer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1

[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao. Semantically self-aligned network for text-toimage part-aware person re-identification. _arXiv preprint_
_arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8_

[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3

[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsapr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1

[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
_Journal of Ambient Intelligence and Humanized Computing,_
pages 1–13, 2022. 3

[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun. Contextual non-local alignment over full-scale representation for text-based person search. _arXiv preprint_
_arXiv:2101.03036, 2021. 8_

[11] Ding Jiang and Mang Ye. Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval. In
_CVPR, 2023. 1, 2, 4, 5, 6, 7, 8_



[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.
Language models are few-shot learners. NeurIPS, 33:1877–
1901, 2020. 3



[3] Yuhao Chen, Guoqing Zhang, Yujiang Lu, Zhenxing Wang,
and Yuhui Zheng. Tipcb: A simple but effective part-based
convolutional baseline for text-based person search. Neuro_computing, 494:171–181, 2022. 8_



[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin,
Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul
Barham, Hyung Won Chung, Charles Sutton, Sebastian
Gehrmann, et al. Palm: Scaling language modeling with
pathways. arXiv preprint arXiv:2204.02311, 2022. 3

[5] Mickael Cormier, Andreas Specker, Julio Junior, CS
Jacques, Lucas Florin, J¨urgen Metzler, Thomas B Moeslund, Kamal Nasrollahi, Sergio Escalera, and J¨urgen Beyerer. Upar challenge: Pedestrian attribute recognition and
attribute-based person retrieval–dataset, design, and results.
In WACV, pages 166–175, 2023. 1



[6] Zefeng Ding, Changxing Ding, Zhiyin Shao, and Dacheng
Tao. Semantically self-aligned network for text-toimage part-aware person re-identification. _arXiv preprint_
_arXiv:2107.12666, 2021. 1, 2, 3, 4, 5, 6, 7, 8_



[7] Dengpan Fu, Dongdong Chen, Hao Yang, Jianmin Bao,
Lu Yuan, Lei Zhang, Houqiang Li, Fang Wen, and Dong
Chen. Large-scale pre-training for person re-identification
with noisy labels. In CVPR, pages 2476–2486, 2022. 3



[8] Hiren Galiyawala, Mehul S Raval, and Dhyey Savaliya. Dsapr: discrete soft biometric attribute-based person retrieval in
surveillance videos. In AVSS, pages 1–7. IEEE, 2021. 1



[9] Hiren Galiyawala, Mehul S Raval, and Meet Patel. Person
retrieval in surveillance videos using attribute recognition.
_Journal of Ambient Intelligence and Humanized Computing,_
pages 1–13, 2022. 3

[10] Chenyang Gao, Guanyu Cai, Xinyang Jiang, Feng Zheng,
Jun Zhang, Yifei Gong, Pai Peng, Xiaowei Guo, and Xing
Sun. Contextual non-local alignment over full-scale representation for text-based person search. _arXiv preprint_
_arXiv:2101.03036, 2021. 8_



[11] Ding Jiang and Mang Ye. Cross-modal implicit relation reasoning and aligning for text-to-image person retrieval. In
_CVPR, 2023. 1, 2, 4, 5, 6, 7, 8_



[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. _arXiv preprint arXiv:1412.6980,_
2014. 6

[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang. Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3



[12] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization. _arXiv preprint arXiv:1412.6980,_
2014. 6

[13] Juncheng Li, Xin He, Longhui Wei, Long Qian, Linchao
Zhu, Lingxi Xie, Yueting Zhuang, Qi Tian, and Siliang
Tang. Fine-grained semantically aligned vision-language
pre-training. NeurIPS, 35:7290–7303, 2022. 3

[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In ICML,
pages 12888–12900. PMLR, 2022.

[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3

[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural language description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8

[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Improving description-based person re-identification by multigranularity image-text alignments. _IEEE TIP, 29:5542–_
5556, 2020. 8

[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language. In ACM MM,
pages 4032–4040, 2020. 8

[19] OpenAI. Chatgpt. [https://openai.com/blog/](https://openai.com/blog/chatgpt/)
[chatgpt/, 2023. 3](https://openai.com/blog/chatgpt/)

[20] OpenAI. Gpt-4 technical report, 2023.

[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.
_NeurIPS, 35:27730–27744, 2022. 3_

[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3

[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8

[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
_Learning Research, 21(1):5485–5551, 2020. 3_

[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image matching. In ICCV, pages 5814–5824, 2019. 8

[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video sequences. In AVSS, pages 1–6. IEEE, 2018. 3

[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
_ACM MM, pages 5566–5574, 2022. 8_



[14] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Blip: Bootstrapping language-image pre-training for unified
vision-language understanding and generation. In ICML,
pages 12888–12900. PMLR, 2022.



[15] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
BLIP-2: bootstrapping language-image pre-training with
frozen image encoders and large language models. In ICML,
2023. 3



[16] Shuang Li, Tong Xiao, Hongsheng Li, Bolei Zhou, Dayu
Yue, and Xiaogang Wang. Person search with natural language description. In CVPR, pages 1970–1979, 2017. 1, 2,
3, 4, 5, 6, 7, 8



[17] Kai Niu, Yan Huang, Wanli Ouyang, and Liang Wang. Improving description-based person re-identification by multigranularity image-text alignments. _IEEE TIP, 29:5542–_
5556, 2020. 8



[18] Kai Niu, Yan Huang, and Liang Wang. Textual dependency
embedding for person search by language. In ACM MM,
pages 4032–4040, 2020. 8



[19] OpenAI. Chatgpt. [https://openai.com/blog/](https://openai.com/blog/chatgpt/)
[chatgpt/, 2023. 3](https://openai.com/blog/chatgpt/)



[20] OpenAI. Gpt-4 technical report, 2023.



[21] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.
_NeurIPS, 35:27730–27744, 2022. 3_



[22] Zhengxin Pan, Fangyu Wu, and Bailing Zhang. Fine-grained
image-text matching by cross-modal hard aligning network.
In CVPR, pages 19275–19284, 2023. 3



[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748–8763, 2021. 4, 6, 7, 8



[24] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee,
Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. Exploring the limits of transfer learning with
a unified text-to-text transformer. The Journal of Machine
_Learning Research, 21(1):5485–5551, 2020. 3_



[25] Nikolaos Sarafianos, Xiang Xu, and Ioannis A Kakadiaris.
Adversarial representation learning for text-to-image matching. In ICCV, pages 5814–5824, 2019. 8



[26] Arne Schumann, Andreas Specker, and J¨urgen Beyerer.
Attribute-based person retrieval and search in video sequences. In AVSS, pages 1–6. IEEE, 2018. 3



[27] Zhiyin Shao, Xinyu Zhang, Meng Fang, Zhifeng Lin, Jian
Wang, and Changxing Ding. Learning granularity-unified
representations for text-to-image person re-identification. In
_ACM MM, pages 5566–5574, 2022. 8_


-----

[28] Rasha Shoitan, Mona M Moussa, and Heba A El Nemr.
Attribute based spatio-temporal person retrieval in video
surveillance. Alexandria Engineering Journal, 63:441–454,
2023. 1

[29] Xiujun Shu, Wei Wen, Haoqian Wu, Keyu Chen, Yiran Song,
Ruizhi Qiao, Bo Ren, and Xiao Wang. See finer, see more:
Implicit modality alignment for text-based person retrieval,
2022. 8

[30] Andreas Specker and J¨urgen Beyerer. Improving attributebased person retrieval by using a calibrated, weighted, and
distribution-based distance metric. In ICIP, pages 2378–
2382. IEEE, 2021. 3

[31] Andreas Specker, Mickael Cormier, and J¨urgen Beyerer.
Upar: Unified pedestrian attribute recognition and person retrieval. In WACV, pages 981–990, 2023. 3

[32] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois,
Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B.
Hashimoto. Stanford alpaca: An instruction-following
[llama model. https://github.com/tatsu-lab/](https://github.com/tatsu-lab/stanford_alpaca)
[stanford_alpaca, 2023. 3](https://github.com/tatsu-lab/stanford_alpaca)

[33] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. Llama: Open and efficient foundation
language models. arXiv preprint arXiv:2302.13971, 2023. 3

[34] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.
Llama 2: Open foundation and fine-tuned chat models. arXiv
_preprint arXiv:2307.09288, 2023. 2, 3_

[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. Attention is all you need. Advances in neural
_information processing systems, 30, 2017. 6_

[36] Xiaohan Wang, Linchao Zhu, Zhedong Zheng, Mingliang
Xu, and Yi Yang. Align and tell: Boosting text-video retrieval with local alignment and fine-grained supervision.
_IEEE TMM, 2022. 3_

[37] Yaodong Wang, Zhenfei Hu, and Zhong Ji. Attribute-wise
reasoning reinforcement learning for pedestrian attribute retrieval. International Journal of Multimedia Information Re_trieval, 12(2):35, 2023. 1_

[38] Zhe Wang, Zhiyuan Fang, Jun Wang, and Yezhou Yang. Vitaa: Visual-textual attributes alignment in person search by
natural language. In ECCV, pages 402–420. Springer, 2020.
8

[39] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Caibc: Capturing all-round information beyond color for text-based person retrieval. In ACM
_MM, pages 5314–5322, 2022. 8_

[40] Zijie Wang, Aichun Zhu, Jingyi Xue, Xili Wan, Chao Liu,
Tian Wang, and Yifeng Li. Look before you leap: Improving text-based person retrieval by learning a consistent crossmodal common manifold. In ACM MM, pages 1984–1992,
2022. 8

[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person reidentification. In CVPR, pages 79–88, 2018. 2



[41] Longhui Wei, Shiliang Zhang, Wen Gao, and Qi Tian.
Person transfer gan to bridge domain gap for person reidentification. In CVPR, pages 79–88, 2018. 2



[42] Yushuang Wu, Zizheng Yan, Xiaoguang Han, Guanbin Li,
Changqing Zou, and Shuguang Cui. Lapscore: languageguided person search via color reasoning. In ICCV, pages
1624–1633, 2021. 8

[43] Shuanglin Yan, Neng Dong, Liyan Zhang, and Jinhui Tang.
Clip-driven fine-grained text-image person re-identification.
_IEEE TIP, 2023. 8_

[44] Lewei Yao, Runhui Huang, Lu Hou, Guansong Lu, Minzhe
Niu, Hang Xu, Xiaodan Liang, Zhenguo Li, Xin Jiang, and
Chunjing Xu. Filip: Fine-grained interactive language-image
pre-training. arXiv preprint arXiv:2111.07783, 2021. 3

[45] Ying Zhang and Huchuan Lu. Deep cross-modal projection
learning for image-text matching. In ECCV, pages 686–701,
2018. 8, 1

[46] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng,
and Wenyu Liu. Fairmot: On the fairness of detection and reidentification in multiple object tracking. IJCV, 129:3069–
3087, 2021. 3

[47] Liang Zheng, Liyue Shen, Lu Tian, Shengjin Wang, Jingdong Wang, and Qi Tian. Scalable person re-identification:
A benchmark. In ICCV, pages 1116–1124, 2015. 2

[48] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench
and chatbot arena, 2023. 3

[49] Zhedong Zheng, Liang Zheng, and Yi Yang. Unlabeled samples generated by gan improve the person re-identification
baseline in vitro. In ICCV, pages 3774–3782, 2017. 2

[50] Zhedong Zheng, Liang Zheng, Michael Garrett, Yi Yang,
Mingliang Xu, and Yi-Dong Shen. Dual-path convolutional
image-text embeddings with instance loss. ACM Transac_tions on Multimedia Computing, Communications, and Ap-_
_plications (TOMM), 16(2):1–23, 2020. 8_

[51] Aichun Zhu, Zijie Wang, Yifeng Li, Xili Wan, Jing Jin,
Tian Wang, Fangqiang Hu, and Gang Hua. Dssl: Deep
surroundings-person separation learning for text-based person retrieval. In ACM MM, pages 209–217, 2021. 1, 2, 3, 4,
5, 6, 7, 8

[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8



[52] Jialong Zuo, Changqian Yu, Nong Sang, and Changxin Gao.
Plip: Language-image pre-training for person representation
learning. arXiv preprint arXiv:2305.08386, 2023. 3, 4, 6, 7,
8


-----

### UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity


### Supplementary Material


#### Contents


In this supplementary material, we will 1) show the details of our proposed cross-modal identity classifying loss
_cid, 2) show more results of the ablation study, and 3)_
_L_
show more specific and compared examples of our proposed
UFineBench and existing other datasets [6, 16, 51].


#### 8. Cross-Modal Identity Classifying


In the training phase of CFAM, we also propose the crossmodal identity classifying loss Lcid as a supplement to
explicitly ensure that the representations of the same image/text pair are closely clustered together.


First, referring to [45], we revisit the traditional identity loss commonly used in the person re-identification task.
Given the extracted embeddings X = {xi}i[N]=1 [and the iden-]
tity labels Y = {yi}i[N]=1[, the traditional identity loss can be]
computed by:


where W yi and W j denote the yi-th and j-th column of
classification weight matrix W, yi indicates the identity label of xi, and byi and bj represent the yi-th and j-th element
of bias vector.


However, this loss only performs identity clustering on
individual modalities, lacking interaction across different
modalities and unable to conduct feature clustering in a
shared cross-modal space. Therefore, we propose the crossmodal identity classifying loss, which not only considers
the cross-modal correlation but also deeply mines hard negative unmatched sample pairs.


Given the extracted visual embeddings = **_vi_** _i=1_
_V_ _{_ _}[N]_
and textual embeddings T = {ti}i[N]=1[, for each][ v][i][ within]
_V, we sample the unpaired textual embedding which owns_
the highest similarity with this vi as the negative. Also,
we sample one hard negative visual embedding for each
**_ti within T in the same way. Specially, we add an extra_**
identity label as the unmatched label, that is, if the original
identity labels are from M classes, the (M + 1)-th identity
will be set as the unmatched label. Through this approach,
we obtain |N _| original positive pairs with original identity_
labels and 2|N _| negative pairs with an unmatched label, de-_
noted as |B| pairs with the identity labels {yi}i[B]=1[.]


Then, for the |B| pairs {vi, ti, yi}i[B]=1[, we first concate-]
nate the visual embedding vi and textual embedding ti to
form the cross-modal embedding zi. Then, for **_zi_** _i=1_
_{_ _}[B]_


Table 9. Ablation study on some components of CFAM. The
“share” denotes whether the granularity decoder is shared across
modalities. The “depth” denotes the number of transformer blocks
in the granularity decoder. The “queries” represents the number of
query tokens used to extract fine-grained information.


with the identity labels {yi}i[B]=1[, the cross-modal identity]
classifying loss can be computed by:


where mlp denotes an MLP layer consisted of a Linear
layer, a LayerNorm, a GELU activation to fuse the crossmodal embeddings more deeply.


#### 9. More Results of Ablation Study


We conduct an ablation experiment to study the influence of
whether the granularity decoder is shared across modalities,
the number of transformer blocks in the granularity decoder
and the number of query tokens. The models are trained and
evaluated on the CUHK-PEDES dataset [16]. According to
the results in the Table 9, we can draw two conclusions as
follows. First, compared to an unshared granularity decoder
(No.1, No.2, No.3), a shared granularity decoder (No.7,
No12, No.17) can bring about a significant improvement
in performance. Second, when the number of transformer
blocks in the granularity decoder and query tokens are 2 and
16 (No.4), respectively, the best performance is obtained,
achieving 72.87% rank-1, 64.92% mAP, and 50.20% mSD,
which is set as the default in CFAM.


-----

1. A young man, with fair skin, who appears to be Caucasian, has a hairstyle that resembles an airplane nose. His hair is
brownish black. He is wearing black-framed glasses and a beard. He is dressed in a gray sweater with a beautiful woman's
pattern printed on it. The sweater is complemented by a black shoulder bag slung over his shoulder. His lower body is clad in
yellow long jeans, and he wears light gray board shoes on his feet. The shoes have a white toe and sole.


2. A young man with fair skin is a white man, with brown-black hair styled like a pilot's cap and wearing black framed glasses
with a mustache. He is wearing a gray sleeveless sweater with a beautiful woman's pattern printed on it, and a black backpack
on his shoulders. He is wearing a pair of yellow, long jeans and a pair of gray canvas shoes with white shoe tips and soles.


1. A young female with a medium-built figure and slender body has relatively white skin. She has long brown hair with a middle
part, the hair is loose. A baseball cap is on her head, the brim and the back half of the hat are black, and the front half of the hat
is white. She is wearing a white T-shirt, with a black English letter on the back. She is wearing black leggings, with the lower part
of her legs showing, and black and pink flat sneakers. In addition, she is carrying a handbag on her right hand, the body of the
bag is dark blue and the handle is white.


2. The young woman has a moderate physique and relatively fair skin. Her dark brown hair is medium in length and spread out.
She wears a duck tongue hat with a black brim and back half, contrasting with a white front half. A white T-shirt covers her
upper half, featuring a line of black English letters on the back. She has on black tight pants that expose her ankles, paired with
black and pink flat sneakers. Her right hand carries a handbag with a dark blue body and a white handle.


1. She was a middle-aged woman, tall and healthy-looking, with pale skin and long black hair. She wore a long-sleeved blue and
white polka dot shirt on her upper body, paired with a red and white striped shoulder bag that looked relatively large. Her lower
body was clad in long black tight pants that reached her ankles, and she wore a pair of black canvas shoes. She held a blue dress.


2. Here is a middle-aged female. She is tall and looks very healthy. Her skin is relatively white, and she has a long black hair. She
is wearing a long-sleeved blue and white polka dot blouse, and carrying a red and white striped single-shoulder bag. It looks like
it has a large capacity. She is wearing a long black compression pants, the length of which has reached her ankle. She is also
wearing a pair of black canvas shoes, and holding a blue piece of clothing.


1. This young woman has a tall and slender figure, with symmetrical features. Her skin is a warm yellow tone. She is wearing a
red cotton T-shirt with short sleeves and white letters on the chest. The cuffs of the T-shirt fall just above her upper arms. Her
lower half is clad in a pair of blue shorts that reach the base of her thighs. She has on a pair of green flip-flops on her feet. Her
face is covered with a black mask, and she holds a mobile phone in her left hand and a light blue vertical armpit bag in her right.

2. There is a young female youth, her exposed skin is slightly yellow, her figure is slender and well-proportioned. She is wearing
a red cotton T-shirt with white English letters on the front. The sleeves of the T-shirt are at her biceps. She is wearing a pair of
short blue casual pants, the legs of the pants reach her knees. She is wearing green canvas slippers. She is wearing a black mask
on her face, her left hand is holding a mobile phone, and her right hand is holding a shallow blue horizontal shoulder bag.


1. This man is a middle-aged individual with a relatively thin build and dark skin, sporting a yellowish-black complexion. He is
bald, with no hair at the front and short black hair remaining on the back and sides. He is wearing a pink shirt with a subtle
black pattern, and the buttons are left unfastened. His lower body is clad in gray long pants, complemented by a black belt. He
has white socks and red shoes on his feet, and he is carrying a white handbag with a green pattern.


2. This is a relatively thin middle-aged man with dark skin, which appears to be yellowish-black. He is a balding man, with no
hair left on the front of his head, only short black remaining hair. His upper body wears a pink shirt with fine black patterns that
seem to be scattered. Several buttons on the shirt are undone. His lower body wears a long gray trousers, and he ties a black
leather belt on his pants. He wears white socks and red shoes, and in his hand he carries a white handbag with green patterns.


Figure 5. Some examples of our proposed UFine6926. Every image has two different fine-grained textual descriptions that describes the
person’s apperance detailedly. Some fine-grained features are highlighted in blue or orange boxes and texts accordingly.


#### 10. More Examples of UFineBench


**Ultra Fine-grained UFine6926.** We have shown more
representative examples of our proposed ultra fine-grained
UFine6926 in Figure 5. As we can see, each person image is annotated with two different textual descriptions that
describes the person’s appearance detailedly. Even if the
external characteristics of certain persons in the images are
very subtle, our textual descriptions do not ignore them like
the previous datasets [6, 16, 51] and portrays these fine

grained features accurately. These fine-grained features are
highlighted in blue or orange boxes and texts in the Figure 5.
For instance, in the bottom example, the area of the person’s
shoes is very inconspicuous, yet our textual description accurately identifies this area and describes it as “white socks
and red shoes.” Meanwhile, we have conducted a statistical
comparison of the word counts per textual description in
our UFine6926 with those in other datasets, as illustrated in
Figure 6. By examining the distribution, we can observe


-----

that the word count for UFine6926 is generally centered
around 80, while simultaneously, CUHK-PEDES [16] and
RSTPReid [51] are both roughly centered around 25, and
ICFG-PEDES [6] is centered around 30. It is evident that
the level of textual detail in our UFine6926 is higher than
that in all other datasets by a significant margin.


**Three Cross Settings in UFine3C. Our proposed UFine3C**
evaluation set has cross domains, cross text granularity and
cross text styles, which is more representative of the challenges faced in real scenarios. (1) Our UFine3C spans
_across various domains. As shown in Figure 7, the images_
in UFine3C have significant variations in resolution, illumination and shooting scenes, which is very close to the situation of images obtained in real scenarios. (2) Our UFine3C
_spans across various text granularity. As shown in Figure 8,_
on the left, the word counts distribution of UFine3C is spanning from coarse-grained to fine-grained. Meanwhile, on
the right, according to the text granularity, the texts can be
categorized into the fine-grained, the medium-grained and
the coarse-grained, respectively. This represents the incon

sistency of granularity within the query texts in real scenarios. (3) Our UFine3C spans across various text styles. As
shown in Figure 9, each image has multiple query texts with
different styles, simulating the language expression styles
of different individuals in practice.


-----

**Fine-grained: This man is middle-aged and has a tall, slender physique. His arms and legs are**
long and lean, and his skin is very fair. He has black hair and is wearing a black top hat and
sunglasses. He was dancing with exaggerated movements, spreading his hands and tiptoeing
on one foot. He is wearing a white V-neck top underneath a black willow top, with white
stitching on the right arm. His lower body is clad in long, black, slim-fitting pants that have
reached the ankle position. He has a black rivet belt tied around his waist, and is wearing white
socks and black leather shoes on his feet.


**Medium-grained: The woman has her black hair tied back in a ponytail and is wearing a sleek**
black blouson jacket, which is paired with matching black trousers. She's also carrying a
backpack slung over her shoulder, completing her stylish and practical outfit.


**Coarse-grained: A woman wearing a dark black jacket with buttons, a pair of black and white**
pants and a pair of white shoes.


**Style1: This man is middle-aged with fair skin and a tall, proportionate build. His short yellow hair is neatly styled. He's**
wearing a blue shirt underneath a beige long-sleeved jacket that reaches his buttocks. The jacket is the perfect length,
and he's paired it with long blue jeans that fall below his ankles. On his feet, he wears brown leather shoes. A black
backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.

**Style2: A middle-aged man with fair skin stands tall an proportional, sporting neat yellow hair and a stylish blue shirt**
under a beige long-sleeved jacket that falls to his buttocks. Paired with long blue jeans that trail below his ankles, he
completes his look with brown leather shoes, a black backpack, and a black DSLR camera hanging around his neck.


**Style3: This is a middle-aged man with fair skin, average height, and well-proportioned build. He has short yellow hair. His**
upper body is wearing a blue shirt, with a beige long-sleeved jacket on top, which reaches just below his butt. His lower
body is wearing a long blue jeans that goes down to just above his ankles. The man's feet are wearing a pair of brown
leather shoes. He is carrying a black backpack on his back. A black single-lens reflex camera hangs around his chest.


**Style4: This man is middle-aged, with a fair complexion and a well-proportioned build. He has short yellow hair and is**
wearing a blue shirt underneath a beige long-sleeved jacket that reaches just below his butt. His lower body is clad in
long blue jeans that stop just above his ankles. On his feet, he wears brown leather shoes. He carries a black backpack on
his back and has a black single-lens reflex camera hanging around his chest.


**Style5: A middle-aged man with fair skin stands before us. His height is average, but his build is well-proportioned. His**
short yellow hair falls neatly across his forehead. His outfit consists of a blue shir worn on his upper body, complemented
by a beige long-sleeved jacket that stops just below his butt. His lower half is covered by long blue jeans that extend
down to just above his ankles. Brown leather shoes grace his feet, providing a touch of sophistication to his overall look.
Carrying a black backpack on his back, he also sports a black single-lens reflex camera hanging around his chest,
indicating a passion for photography.


**Style6: The man is tall and slender, with a build that's well-proportioned. He has fair skin and short, yellow hair that's**
styled neatly. He's wearing a blue shirt underneath a beige long-sleeved jacket that fits him perfectly and reaches his
buttocks. The jacket is paired with long blue jeans that fall below his ankles, and he's wearing brown leather shoes. A
black backpack rests comfortably on his back, and a black DSLR camera hangs around his neck.


Figure 9. Our UFine3C spans across various text styles, simulating the language expression styles of different individuals.


(b) Textual Descriptions with Different Granularity


-----

